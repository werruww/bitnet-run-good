{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrTBuCScLoc9"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/shumingma/transformers.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/microsoft/bitnet-b1.58-2B-4T"
      ],
      "metadata": {
        "id": "BVu9iSwVMQtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Specify the model path\n",
        "model_path = \"/content/bitnet-b1.58-2B-4T\"\n",
        "\n",
        "# Load tokenizer (still from the model ID)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/bitnet-b1.58-2B-4T\")\n",
        "\n",
        "# Load model from the specified path\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# ... (rest of your code remains the same)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "59faKB1eMVQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the chat template\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"How are you?\"},\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "chat_input = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate response\n",
        "chat_outputs = model.generate(**chat_input, max_new_tokens=11)\n",
        "response = tokenizer.decode(chat_outputs[0][chat_input['input_ids'].shape[-1]:], skip_special_tokens=True) # Decode only the response part\n",
        "print(\"\\nAssistant Response:\", response)"
      ],
      "metadata": {
        "id": "N36gJ2PMM05X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZEZXMSeONafe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VJNMTy-wNach"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b84LbFIsNaZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### /content/bitnet-b1.58-2B-4T/configuration_bitnet.py"
      ],
      "metadata": {
        "id": "HwFiK7HiNfLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#/content/bitnet-b1.58-2B-4T/configuration_bitnet.py\n",
        "\n",
        "\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n",
        "#\n",
        "# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n",
        "# and OPT implementations in this library. It has been modified from its\n",
        "# original forms to accommodate minor architectural differences compared\n",
        "# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" LLaMA model configuration\"\"\"\n",
        "\n",
        "from transformers.configuration_utils import PretrainedConfig\n",
        "from transformers.utils import logging\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "LLAMA_PRETRAINED_CONFIG_ARCHIVE_MAP = {}\n",
        "\n",
        "\n",
        "class BitnetConfig(PretrainedConfig):\n",
        "    r\"\"\"\n",
        "    This is the configuration class to store the configuration of a [`BitnetModel`]. It is used to instantiate an LLaMA\n",
        "    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n",
        "    defaults will yield a similar configuration to that of the LLaMA-7B.\n",
        "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
        "    documentation from [`PretrainedConfig`] for more information.\n",
        "    Args:\n",
        "        vocab_size (`int`, *optional*, defaults to 32000):\n",
        "            Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n",
        "            `inputs_ids` passed when calling [`BitnetModel`]\n",
        "        hidden_size (`int`, *optional*, defaults to 4096):\n",
        "            Dimension of the hidden representations.\n",
        "        intermediate_size (`int`, *optional*, defaults to 11008):\n",
        "            Dimension of the MLP representations.\n",
        "        num_hidden_layers (`int`, *optional*, defaults to 32):\n",
        "            Number of hidden layers in the Transformer decoder.\n",
        "        num_attention_heads (`int`, *optional*, defaults to 32):\n",
        "            Number of attention heads for each attention layer in the Transformer decoder.\n",
        "        num_key_value_heads (`int`, *optional*):\n",
        "            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n",
        "            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n",
        "            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n",
        "            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n",
        "            by meanpooling all the original heads within that group. For more details checkout [this\n",
        "            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n",
        "            `num_attention_heads`.\n",
        "        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n",
        "            The non-linear activation function (function or string) in the decoder.\n",
        "        max_position_embeddings (`int`, *optional*, defaults to 2048):\n",
        "            The maximum sequence length that this model might ever be used with. Bitnet 1 supports up to 2048 tokens,\n",
        "            Bitnet 2 up to 4096, CodeBitnet up to 16384.\n",
        "        initializer_range (`float`, *optional*, defaults to 0.02):\n",
        "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
        "        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n",
        "            The epsilon used by the rms normalization layers.\n",
        "        use_cache (`bool`, *optional*, defaults to `True`):\n",
        "            Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
        "            relevant if `config.is_decoder=True`.\n",
        "        pad_token_id (`int`, *optional*):\n",
        "            Padding token id.\n",
        "        bos_token_id (`int`, *optional*, defaults to 1):\n",
        "            Beginning of stream token id.\n",
        "        eos_token_id (`int`, *optional*, defaults to 2):\n",
        "            End of stream token id.\n",
        "        pretraining_tp (`int`, *optional*, defaults to 1):\n",
        "            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n",
        "            document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to understand more about it. This value is\n",
        "            necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n",
        "            issue](https://github.com/pytorch/pytorch/issues/76232).\n",
        "        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n",
        "            Whether to tie weight embeddings\n",
        "        rope_theta (`float`, *optional*, defaults to 10000.0):\n",
        "            The base period of the RoPE embeddings.\n",
        "        rope_scaling (`Dict`, *optional*):\n",
        "            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n",
        "            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n",
        "            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n",
        "            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n",
        "            these scaling strategies behave:\n",
        "            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n",
        "            experimental feature, subject to breaking API changes in future versions.\n",
        "        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n",
        "            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n",
        "        attention_dropout (`float`, *optional*, defaults to 0.0):\n",
        "            The dropout ratio for the attention probabilities.\n",
        "    ```python\n",
        "    >>> from transformers import BitnetModel, BitnetConfig\n",
        "    >>> # Initializing a LLaMA llama-7b style configuration\n",
        "    >>> configuration = BitnetConfig()\n",
        "    >>> # Initializing a model from the llama-7b style configuration\n",
        "    >>> model = BitnetModel(configuration)\n",
        "    >>> # Accessing the model configuration\n",
        "    >>> configuration = model.config\n",
        "    ```\"\"\"\n",
        "\n",
        "    model_type = \"llama\"\n",
        "    keys_to_ignore_at_inference = [\"past_key_values\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=32000,\n",
        "        hidden_size=4096,\n",
        "        intermediate_size=11008,\n",
        "        num_hidden_layers=32,\n",
        "        num_attention_heads=32,\n",
        "        num_key_value_heads=None,\n",
        "        hidden_act=\"silu\",\n",
        "        max_position_embeddings=2048,\n",
        "        initializer_range=0.02,\n",
        "        rms_norm_eps=1e-6,\n",
        "        use_cache=True,\n",
        "        pad_token_id=None,\n",
        "        bos_token_id=1,\n",
        "        eos_token_id=2,\n",
        "        pretraining_tp=1,\n",
        "        tie_word_embeddings=False,\n",
        "        rope_theta=10000.0,\n",
        "        rope_scaling=None,\n",
        "        attention_bias=False,\n",
        "        attention_dropout=0.0,\n",
        "        weight_bits=1,\n",
        "        input_bits=8,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.hidden_size = hidden_size\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "\n",
        "        # for backward compatibility\n",
        "        if num_key_value_heads is None:\n",
        "            num_key_value_heads = num_attention_heads\n",
        "\n",
        "        self.num_key_value_heads = num_key_value_heads\n",
        "        self.hidden_act = hidden_act\n",
        "        self.initializer_range = initializer_range\n",
        "        self.rms_norm_eps = rms_norm_eps\n",
        "        self.pretraining_tp = pretraining_tp\n",
        "        self.use_cache = use_cache\n",
        "        self.rope_theta = rope_theta\n",
        "        self.rope_scaling = rope_scaling\n",
        "        self._rope_scaling_validation()\n",
        "        self.attention_bias = attention_bias\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.weight_bits = weight_bits\n",
        "        self.input_bits = input_bits\n",
        "\n",
        "        super().__init__(\n",
        "            pad_token_id=pad_token_id,\n",
        "            bos_token_id=bos_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            tie_word_embeddings=tie_word_embeddings,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    def _rope_scaling_validation(self):\n",
        "        \"\"\"\n",
        "        Validate the `rope_scaling` configuration.\n",
        "        \"\"\"\n",
        "        if self.rope_scaling is None:\n",
        "            return\n",
        "\n",
        "        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n",
        "            raise ValueError(\n",
        "                \"`rope_scaling` must be a dictionary with with two fields, `type` and `factor`, \"\n",
        "                f\"got {self.rope_scaling}\"\n",
        "            )\n",
        "        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n",
        "        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n",
        "        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n",
        "            raise ValueError(\n",
        "                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n",
        "            )\n",
        "        if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n",
        "            raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")"
      ],
      "metadata": {
        "id": "lV4hzw2gNaVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XFKEL8BwOUwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vI3lY0nGOUtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "daH9gOVmOUqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, upload_folder\n",
        "\n",
        "# إعدادات المستخدم\n",
        "token = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
        "repo_id = \"rakmik/bitnetrun\"\n",
        "folder_path = \"/content/bitnet-b1.58-2B-4T\"  # ← غير هذا إلى مسار مجلدك المحلي\n",
        "\n",
        "# رفع المجلد\n",
        "upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    folder_path=folder_path,\n",
        "    path_in_repo=\"\",  # يمكنك تحديد مجلد داخل الريبو إذا رغبت\n",
        "    repo_type=\"model\",  # أو \"dataset\" إذا كان مستودع بيانات\n",
        "    token=token,\n",
        ")\n",
        "print(\"✅ تم رفع المجلد بنجاح إلى:\", repo_id)\n"
      ],
      "metadata": {
        "id": "sfyA7o8QOUmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KNXNZ1p5OvjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h8RuKKlXOvgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nHpvo5gdOvdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"rakmik/bitnetrun\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Apply the chat template\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"How are you?\"},\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "chat_input = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate response\n",
        "chat_outputs = model.generate(**chat_input, max_new_tokens=11)\n",
        "response = tokenizer.decode(chat_outputs[0][chat_input['input_ids'].shape[-1]:], skip_special_tokens=True) # Decode only the response part\n",
        "print(\"\\nAssistant Response:\", response)\n"
      ],
      "metadata": {
        "id": "IwRkeG_LOvZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NXJeAGiXPfte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jBg3sDAVPfq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ixmz7iEVPfnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the chat template\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"how is python?\"},\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "chat_input = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate response\n",
        "chat_outputs = model.generate(**chat_input, max_new_tokens=55)\n",
        "response = tokenizer.decode(chat_outputs[0][chat_input['input_ids'].shape[-1]:], skip_special_tokens=True) # Decode only the response part\n",
        "print(\"\\nAssistant Response:\", response)"
      ],
      "metadata": {
        "id": "T_T9JhfsPfjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IecvUaqUPlXZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}